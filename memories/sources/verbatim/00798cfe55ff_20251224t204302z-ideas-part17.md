---
title: 20251224t204302z-ideas-part17
created: 2025-12-24T20:47:07Z
tags: source, verbatim
importance: 0
source: sources_ingest
source_id: 00798cfe55ff
original_path: pasted/20251224t204302z_ideas_part17.md
source_set: ideas

source_set_context: things i am learning like social science etc
---

- site:: [The Guardian]([https://www.theguardian.com/commentisfree/2023/dec/15/slovenia-refugees-independence-bosnia-far-right](https://www.theguardian.com/commentisfree/2023/dec/15/slovenia-refugees-independence-bosnia-far-right))
          - author:: Ana Schnabl
          - date-published:: [[2023-12-15]]
        - ### Highlights
          - > What I do find is a legion of Slovene-speaking men and women who see no issue in dangerous far-right ideology of the kind espoused by  the leader of the biggest hard-right party. #k
            - {{cloze Janez Janša}} 
              - Quite the contrary: I find their many online likes – their applause, their racism and their Islamophobia – proof that the myth of Slovene-ness and the myth of [Slovene independence](https://www.theguardian.com/world/1991/jun/26/eu.politics) that nationalists seeded have successfully germinated. _Grown_. I, essentially, find a lot of hate, propelled by fear. This is [fear](https://balkaninsight.com/2023/10/23/slovenia-croatia-border-controls-will-likely-be-extended-minister) that nationalists have managed to whip up despite our country not having very many[ immigrant workers, refugees ](https://www.oecd-ilibrary.org/sites/671495d1-en/index.html?itemId=/content/component/671495d1-en)or even citizens who were born outside the Balkan region. Such is their grip on public imagination. [⤴️](https://omnivore.app/me/slovenia-and-i-grew-up-together-and-i-ve-seen-its-early-dream-of-18c6d43a852#4fe52a1b-1062-4e8c-98ff-d4d6df6a9590)
      - [Call for Paper Abstracts - Special Issue on “Advances in Causal Systems Mapping” - CECAN](https://omnivore.app/me/call-for-paper-abstracts-special-issue-on-advances-in-causal-sys-18c6d2c381d) [[Omnivore]]
        - site:: [CECAN](https://www.cecan.ac.uk/news/call-for-paper-abstracts-special-issue-on-advances-in-causal-systems-mapping/)
          - author:: Gail Deason
          - date-published:: [[2023-12-14]]
      - [How to Regulate Unsecured “Open-Source” AI: No Exemptions | TechPolicy.Press]([https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb)) [[Omnivore]]
        - site:: [Tech Policy Press](https://www.techpolicy.press/how-to-regulate-unsecured-opensource-ai-no-exemptions/)
          - author:: [[gjDavid Evan Harris]]
          - date-published:: [[2023-12-04]]
        - ### Highlights
          - White House’\s recent AI Executive Order does not mention the term “open source,” but instead [uses the related, and more specific term]  #kk
            - “Dual-Use Foundation Models with Widely Available Model Weights.” The term “dual-use” refers to the fact that these models have both civilian and military applications. “Foundation models” are general purpose AI models that can be used in a wide variety of ways, including creation or analysis of words, images, audio, video or even design chemical or biological outputs. The Executive Order states, “When the weights for a dual-use foundation model are widely available — such as when they are publicly posted on the Internet [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#133cc33a-009d-4214-b5fe-7a97711d9a24)
          - > Open source is also not yet a [clearly-defined](https://blog.opensource.org/towards-a-definition-of-open-artificial-intelligence-first-meeting-recap/) term for AI, with some rightly pointing out that openness is a [spectrum](https://arxiv.org/abs/2302.04844), not a binary distinction, and [this debate](https://news.ycombinator.com/item?id=36815255) may not be over any time soon. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#4a7457e0-ecdb-46d6-9f1f-82754a4486f9)
          - > such, for both convenience and clarity I will use the term “unsecured” as shorthand for this accurate-if-not-svelte term from the Executive Order. “Unsecured” is intended to convey both the literal choice to not secure the weights of these AI systems and also the threat to security posed by these systems. Readers can, as such, also think of this article [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#b7675930-ee6d-48d5-970b-bf1ae0f5be09)
          - > Enter the unsecured models. Most famous is Meta’s Llama 2\. It was released by Meta with a 27-page “[Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/),” which was promptly ignored by the creators of “[Llama 2 Uncensored](https://huggingface.co/jarradh/llama2%5F70b%5Fchat%5Funcensored),” a derivative model with safety features stripped away, and hosted for free download on the Hugging Face AI repository. One of my undergraduate students at Berkeley shared with me that they were able to install it in 15 minutes on a MacBook Pro laptop (with an [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#cc7936ae-0475-4079-ad27-dba2ffedf7b4)
          - > Once someone releases an “uncensored” version of an unsecured AI system, the original maker of the unsecured system is largely powerless to do anything about it. The maker of the original model could request that it be taken down from certain hosting sites, but if it’s powerful, it is still likely to continue circulating online. Under current law, it is unclear at best whether anyone is liable for any wrongdoing that is enabled by these models. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#b5de6ab5-fec6-4aee-b147-92b14324d7f9)
          - > Deception is another key concern with disturbing potential. The [Executive Order](https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.pi33osdg8k1e) describes this harm as “permitting the evasion of human control or oversight through means of deception or obfuscation.” This type of concern is not purely speculative—it has been [observed](https://theconversation.com/ai-systems-have-learned-how-to-deceive-humans-what-does-that-mean-for-our-future-212197) in an AI system called CICERO, designed by Meta to play the game Diplomacy, as well as in GPT-4\. An unsecured version of CICERO was released by Meta. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#266db234-afb8-47a1-95b1-604ab5c098d9)
          - > Just as is the case with the logic of nuclear nonproliferation, just because you can’t get rid of all the nuclear weapons in the world doesn’t mean you shouldn’t try to keep them in as few hands as possible. [⤴️]([https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#a4a35149-52e5-4087-921f-6f6a0e1f061c](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#a4a35149-52e5-4087-921f-6f6a0e1f061c))
            - > seen in [[Slovakia]] ’s recent very close election
            - the outcome of which may have been influenced by the release of an audio deepfake of the losing candidate purportedly discussing vote buying. The winner and beneficiary of the deepfake was in favor of withdrawing military support from neighboring Ukraine. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#11c4ca3d-678d-40b0-a472-6f5ff785298c)
          - > ### **Regulatory Action: AI Systems**
            - >
              - 1. **Pause unsecured AI releases to adopt best practices and secure Distribution Channels and Attack Surfaces** – Pause all new releases of unsecured AI systems until developers have met requirements below, and in ways that safety features cannot be easily removed by bad actors with significantly less effort or cost than it would take to train a similar new model. During this pause, provide a legally binding deadline for all major Distribution Channels and Attack Surfaces to meet the requirements under 2\. below.
              - 2. **Registration & Licensing –** Require retroactive and ongoing registration [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#29a30f7f-4d56-4de8-9e5a-81386fe29a08)
          - > discussions with technical experts and policymakers, as well as [recent recommendations from CSET](https://cset.georgetown.edu/article/regulating-the-ai-frontier-design-choices-and-constraints/#Compute), criteria could include something along the lines of:
            - Models trained using a quantity of computing power greater than 10^26 integer or floating-point operations, or using primarily biological sequence data and using a quantity of computing power greater than 10^23.
              - Greater than $100,000 training cost.
              - Greater than 10 billion parameters (smaller models are currently becoming more capable).
              - Higher performance on one or more standardized capabilities [evaluations](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) or [evaluations focused specifically on risk levels](https://arxiv.org/pdf/2305.15324.pdf) than current models.
              - Capable of producing highly realistic synthetic media images, audio and video. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#bffab904-6fa0-40c2-a556-3e0d7373054c)
          - > **Liability for “Reasonably Foreseeable Misuse” and Negligence** – Hold developers of AI systems legally liable for harms caused by their systems, including harms to individuals and harms to society. The recently signed [Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) states that actors developing AI systems “which are unusually powerful and potentially harmful, have a particularly strong responsibility for ensuring the safety of these AI systems.” Establishing this liability in a binding way could be based on the principle that “reasonably foreseeable misuse” would include all of the risks discussed in this article. This concept is referenced in the [EU’s draft AI Act](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236%5FEN.pdf) (in multiple locations) and [Cyber Resilience Act](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex:52022PC0454) (CRA - Chapter 1, Article 3 (26)). Though these laws are not finalized, and the way that the liability mechanism would function is not yet clear, the Linux Foundation is already [telling developers to prepare](https://www.linuxfoundation.org/blog/understanding-the-cyber-resilience-act) for the CRA to apply to open-source software developed by private companies. [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#93668a8e-2f1f-44c1-9bfb-4a6223399268)
          - > **Require Provenance and Watermarking Best Practices –** The [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#5078b88f-99e7-4139-a1e6-62b2672c37c3)
          - > * important also to assure that no AI system can be approved for distribution if it can be abused to remove watermarks from other content.
            - All AI systems that do not use robust provenance and watermarking best practices by a set deadline in the coming months should be shut down [⤴️](https://omnivore.app/me/how-to-regulate-unsecured-open-source-ai-no-exemptions-tech-poli-18c58f6a8cb#a1b239e4-2be0-4a2d-8681-475dc7bd916d)
          - > Watermarking will probably never be foolproof—it is  #kk
