---
title: 20251224t204302z-ideas-part40
created: 2025-12-24T20:52:07Z
tags: source, verbatim
importance: 0
source: sources_ingest
source_id: 89af2a089faa
original_path: pasted/20251224t204302z_ideas_part40.md
source_set: ideas

source_set_context: things i am learning like social science etc
---

Imagine you have GPT-7, and it‚Äôs starting to become superhuman at many tasks. It‚Äôs hooked up to a bunch of tools and the internet. You want to use it to help run your business, and it proposes a very complicated series of action and computer code. You want to know‚Äîwill this plan violate any laws? Current alignment techniques rely on human supervision. The problem is that as these models become superhuman, humans won‚Äôt be able to reliably supervise their outputs. (In this example, the series of actions is too complicated for humans to be able to fully understand the consequences.). And if you can‚Äôt reliably detect bad behavior, you can‚Äôt reliably prevent bad behavior.
Add notes to this highlight...

==Fundamentally, I think AI alignment is an ML problem.== As AI systems are becoming more advanced, alignment is increasingly becoming a ‚Äúreal science,‚Äù where we can do ML experiments, rather than just thought experiments. I think this is really different compared to 5 years ago."
        - 
    - encryption / privacy 
    - Prompting techniques
      -  [What We Learned from a Year of Building with LLMs (Part I) ‚Äì O‚ÄôReilly - omnivore.app](https://omnivore.app/stevepowell99/what-we-learned-from-a-year-of-building-with-ll-ms-part-i-o-reil-1902cce2dcb)
        - individual prompts are good, but #kkkk
          - breaking down into steps is better, but then you always end with something too ocmplicated. so best = hardcoding into separate prompts
            - We recommend ==starting with prompting ==when developing new applications. It‚Äôs easy to both underestimate and overestimate its importance. It‚Äôs underestimated because the right prompting techniques, when used correctly, can get us very far. It‚Äôs overestimated because even prompt-based applications require significant engineering around the prompt to work well.
Add notes to this highlight...

==The idea of in-context learning via n-shot prompts is to provide the LLM with a few examples that demonstrate the task and align outputs to our expectations. A few tips:
If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ‚â• 5. Don‚Äôt be afraid to go as high as a few dozen==.
Add notes to this highlight...

For example, when asking an LLM to summarize a meeting transcript, we can be ==explicit about the steps, ==such as:
First, list the key decisions, follow-up items, and associated owners in a sketchpad.
Then, check that the details in the sketchpad are factually consistent with the transcript.
Finally, synthesize the key points into a concise summary.
Recently==, some doubt has been cast on whether this technique is as powerful ==as believed. Additionally, there‚Äôs significant debate about exactly what happens during inference when chain-of-thought is used. Regardless, this technique is one to experiment with when possible.
Add notes to this highlight...

prompt typically starts simple: A few sentences of instruction, a couple of examples, and we‚Äôre good to go. But as we try to improve performance and handle more edge cases, complexity creeps in. More instructions. Multi-step reasoning. Dozens of examples==. Before we know it, our initially simple prompt is now a 2,000 token frankenstein. And to add injury to insult, it has worse performance on the more common and straightforward inputs! GoDaddy shared this challenge as their No. 1 lesson from building with LLMs.==
Add notes to this highlight...

==Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps to:
Extract key decisions, action items, and owners into structured format
Check extracted details against the original transcription for consistency
==Generate a concise summary from the structured details
As a result, we‚Äôve split our single prompt into multiple prompts that are each siple, focused, and easy to understand. And by breaking them up, we can now iterate and eval each prompt individually.
Add notes to this highlight...

The other key optimization is #kkk
              - vthe structure of your context. Your bag-of-docs representation isn‚Äôt helpful for humans, don‚Äôt assume it‚Äôs any good for agents.
Add notes to this highlight...

Finally, consider the level of detail provided in the document. Imagine we‚Äôre building a RAG system to generate SQL queries from natural language. We could simply provide table schemas with column names as context. But, what if we include column descriptions and some representative values? The additional detail could help the LLM better understand the semantics of the table and thus generate more correct SQL.
Add notes to this highlight...

              - 
            - while embeddings are undoubtedly a powerful tool,#kkk
              -  they are not the be all and end all. First, while they excel at capturing high-level semantic similarity, ==they may struggle with more specific, keyword-based queries==, like when users search for names (e.g., Ilya), acronyms (e.g., RAG), or IDs (e.g., claude-3-sonnet). Keyword-based search, such as BM25, are explicitly designed for this. And after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isn‚Äôt being returned.
==Vector embeddings do not magically solve search==. In fact, the heavy lifting is in the step before you re-rank with semantic similarity search. Making a genuine improvement over BM25 or full-text search is hard.

Add notes to this highlight...

==Nearest Neighbor Search with naive embeddings yields very noisy results ==and you‚Äôre likely better off starting with a keyword-based approach.
Add notes to this highlight...

Prefer RAG over fine-tuning for new knowledge
Add notes to this highlight...

Beyond improved performance, RAG comes with several practical advantages too. First, compared to continuous pretraining or fine-tuning, it‚Äôs easier‚Äîand cheaper!‚Äîto keep retrieval indices up-to-date. Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents. In addition, the R in RAG provides finer grained control over how we retrieve documents. For example, if we‚Äôre hosting a RAG system for multiple organizations, by partitioning the retrieval indices, we can ensure that each organization can only retrieve documents from their own index.
Add notes to this highlight...

While ==it‚Äôs true that long contexts will be a game-changer for use cases such as analyzing multiple documents or chatting with PDFs, #kkk==
                - ==the rumors of RAG‚Äôs demise are greatly exaggerated. First, even with a context window of 10M tokens, we‚Äôd still need a way to select information to feed into the model==. Second, ==beyond the narrow needle-in-a-haystack eval, we‚Äôve yet to see convincing data that models can effectively **reason** over such a large context.==
Add notes to this highlight...

Prompting an LLM is just the beginning. To get the most juice out of them, we need to think beyond a single prompt and embrace workflows. For example, how could we split a single complex task into multiple simpler tasks? When is
Add notes to this highlight...

Step-by-step, multi-turn ‚Äúflows‚Äù can give large boosts. We already know that by decomposing a single big prompt into multiple smaller prompts, we can achieve better results. An example of this is AlphaCodium: By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%. The workflow includes:
Reflecting on the problem
Reasoning on the public tests
Generating possible solutions
Ranking possible solutions
Generating synthetic tests
Iterating on the solutions on public and synthetic tests.
Small tasks with clear objectives make for the best agent or flow prompts. It‚Äôs not required that every agent prompt requests structured output, but structured outputs help a lot to interface with whatever system is orchestrating the agent‚Äôs interactions with the environment.
Add notes to this highlight...

While AI agents can dynamically react to user requests and the environment,#kkk
                  - == their non-deterministic nature makes them a challenge to deploy. Each step an agent takes has a chance of failing, and the chances of recovering from the error are poor. ==Thus, the likelihood that an agent completes a multi-step task successfully decreases exponentially as the number of steps increases. As a result, teams building agents find it difficult to deploy reliable agents. A promising approach is #kkk
                    - ==to have agent systems that produce deterministic plans which are then executed in a structured, reproducible way.
==Add notes to this highlight...

==Create unit tests (i.e., assertions) consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria. While three criteria might seem ==arbitrary, it‚Äôs a practical number to start with; fewer might indicate that your task isn‚Äôt sufficiently defined or is too open-ended, like a general-purpose chatbot. These
Add notes to this highlight...

LLM-as-Judge, #kkk
                      - ==where we use a strong LLM to evaluate the output of other LLMs, has been met with skepticism by some. (Some of us were initially huge skeptics.) Nonetheless, when implemented well, LLM-as-Judge achieves decent ==correlation with human judgements, and can at least help build priors about how a new prompt or technique may perform.
Add notes to this highlight...

Here are some suggestions to get the most out of LLM-as-Judge:#kkk
                        - ==Use pairwise comparisons==: Instead of asking the LLM to score a single output on a Likert scale, present it with two options and ask it to select the better one. This tends to lead to more stable results."
    - SpaceY #kkk
      - it's all about breaking stuff down into entities. so last year.
    - topic models
      - similar to clustering
      - articles are a distribution over topics, and topics are a distribution over words.
      - topics are normally represented by their say top 10 typical words
      - 
      - in dynamic topic modelling, the topics can gradually change over time.
      - Latent Dirichlet Allocation LDA 
      - 
        - Blei & Rafferty. Dynamic [topic models](https://workflowy.com/#/0573d34bc9cf) 
    - Dunivin #kkk
      - chain of thought reasoning in qual coding matches human performance with 4, not 3.5
      - many things we found
        - yes break the task up
  - what do you call the reason for being thorough about implementing laws #kkk
    - moral hazard
  - Systems
    - üèõÔ∏è Examples of Systems Thinking in Political Science1. Policy Feedback Loops and "Lock-In"This approach, most associated with historian and political scientist **Paul Pierson**, is a direct answer to your "river" question.#kkkkk 
      - **The "Meta-Message":** "Politics is complicated."
      - **The Practical Application (The "River"):** Pierson argues that policies, once they are created, don't just *respond* to politics; they actively *create* new politics. They are "rivers" that, once carved, are incredibly difficult to re-route.
      - **A Concrete Example: Social Security in the US.**
        - **Initial State (Wet Landscape):** In the early 20th century, support for the elderly was fragmented (family, local charities).
        - **Initial Trickle (The Policy):** The Social Security Act of 1935 is passed.
        - **The Feedback Loop (Carving the Channel):** The policy creates a new, massive, and highly motivated political group: **senior citizens** who receive (and expect) these benefits. This group organizes into powerful lobbies (like the AARP).
