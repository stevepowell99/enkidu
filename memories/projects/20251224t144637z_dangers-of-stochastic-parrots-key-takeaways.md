---
title: Dangers of Stochastic Parrots: Key Takeaways
created: 2025-12-24T14:46:37Z
tags: NLP,bias,ethics,environmental-cost,datasets,language-models
importance: 2
source: sources_ingest
source_ref: memories/sources/verbatim/d0fb84e8a088_benderdangersstochasticparrots2021.md
original_path: content/zotero/benderDangersStochasticParrots2021.md
---

- The environmental cost of very large language models is a primary concern; scaling up increases compute and carbon footprint.
- Data collection at scale risks documentation debt; budgets for curation and documentation should be allocated from the start of a project.
- LMs do not truly understand language; larger models often improve tasks that manipulate linguistic form rather than underlying meaning.
- Bias and representation persist; current data practices can underrepresent or misrepresent marginalized communities and language varieties.
- Recommendations include reporting training time and hyperparameter sensitivity, and investing in compute clouds to improve access and reproducibility.

Source: content/zotero/benderDangersStochasticParrots2021.md

Why: To remember the key cautions around scaling language models and data practices.
